# SerdesAI ğŸ¦€

> **A complete Rust port of pydantic-ai for building production-ready AI agents**

[![Crates.io](https://img.shields.io/crates/v/serdes-ai.svg)](https://crates.io/crates/serdes-ai)
[![Documentation](https://docs.rs/serdes-ai/badge.svg)](https://docs.rs/serdes-ai)
[![CI](https://github.com/janfeddersen-wq/serdesAI/workflows/CI/badge.svg)](https://github.com/janfeddersen-wq/serdesAI/actions)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

SerdesAI is a comprehensive, type-safe Rust framework for building AI agents that interact with large language models. It is a complete port of [pydantic-ai](https://github.com/pydantic/pydantic-ai) to pure Rust, providing ergonomic async APIs with compile-time safety guarantees.

## âœ¨ Features

- ğŸ¤– **Type-safe Agents** - Generic over dependencies and output types with compile-time validation
- ğŸ”Œ **Multi-provider Support** - OpenAI, Anthropic, Google Gemini, Groq, Mistral, Ollama, AWS Bedrock, Azure OpenAI
- ğŸ› ï¸ **Tool Calling** - Define tools with automatic JSON schema generation via macros
- ğŸ“¡ **Streaming** - Real-time response streaming with backpressure support
- ğŸ”„ **Smart Retries** - Configurable retry strategies with exponential backoff
- ğŸ“Š **Evaluations** - Built-in testing and benchmarking framework
- ğŸ”€ **Graph Workflows** - Complex multi-agent orchestration with state management
- ğŸ”— **MCP Support** - Model Context Protocol integration for tool servers
- ğŸ“ **Structured Output** - JSON schema-based output validation with serde
- ğŸ” **Embeddings** - Semantic search and RAG support

## ğŸš€ Quick Start

Add to your `Cargo.toml`:

```toml
[dependencies]
serdes-ai = "0.1"
tokio = { version = "1", features = ["full"] }
```

### Simple Chat

```rust
use serdes_ai::prelude::*;
use serdes_ai::OpenAIChatModel;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let agent = Agent::new(OpenAIChatModel::from_env("gpt-4o")?)
        .system_prompt("You are a helpful assistant.")
        .build();
    
    let result = agent.run("Hello! What can you help me with?", ()).await?;
    println!("{}", result.output);
    
    Ok(())
}
```

### Tool Calling

```rust
use serdes_ai::prelude::*;
use serdes_ai_tools::{Tool, ToolDefinition, ToolReturn, ToolResult, SchemaBuilder};

struct CalculatorTool;

impl Tool<()> for CalculatorTool {
    fn definition(&self) -> ToolDefinition {
        ToolDefinition::new("calculate", "Perform arithmetic calculations")
            .with_parameters(
                SchemaBuilder::new()
                    .string("expression", "Math expression to evaluate", true)
                    .build()
                    .unwrap()
            )
    }
    
    async fn call(
        &self, 
        _ctx: &RunContext<()>, 
        args: serde_json::Value
    ) -> ToolResult {
        let expr = args["expression"].as_str().unwrap();
        // Evaluate the expression...
        Ok(ToolReturn::text("42"))
    }
}

let agent = Agent::new(model)
    .tool(CalculatorTool)
    .build();
```

### Structured Output

```rust
use serdes_ai::prelude::*;
use serdes_ai_macros::Output;
use serde::{Deserialize, Serialize};

#[derive(Debug, Serialize, Deserialize, Output)]
struct PersonInfo {
    name: String,
    age: u32,
    occupation: String,
}

let agent = Agent::new(model)
    .output_type::<PersonInfo>()
    .build();

let result = agent.run("John is a 30 year old engineer", ()).await?;
println!("Extracted: {} is {} and works as {}", 
    result.output.name, 
    result.output.age,
    result.output.occupation
);
```

### Streaming Responses

```rust
use serdes_ai::prelude::*;
use serdes_ai_streaming::AgentStreamEvent;
use futures::StreamExt;

let mut stream = agent.run_stream("Write a poem", ()).await?;

while let Some(event) = stream.next().await {
    if let AgentStreamEvent::TextDelta { content, .. } = event {
        print!("{}", content);
    }
}
```

### Graph-based Workflows

```rust
use serdes_ai::prelude::*;
use serdes_ai_graph::{Graph, BaseNode, NodeResult, GraphRunContext, GraphResult};
use async_trait::async_trait;

#[derive(Debug, Clone, Default)]
struct WorkflowState {
    query: String,
    research: Option<String>,
    response: Option<String>,
}

struct ResearchNode;
struct WriteNode;

#[async_trait]
impl BaseNode<WorkflowState, (), String> for ResearchNode {
    fn name(&self) -> &str { "research" }
    
    async fn run(
        &self,
        ctx: &mut GraphRunContext<WorkflowState, ()>,
    ) -> GraphResult<NodeResult<WorkflowState, (), String>> {
        ctx.state.research = Some(format!("Research for: {}", ctx.state.query));
        Ok(NodeResult::next(WriteNode))
    }
}

#[async_trait]
impl BaseNode<WorkflowState, (), String> for WriteNode {
    fn name(&self) -> &str { "write" }
    
    async fn run(
        &self,
        ctx: &mut GraphRunContext<WorkflowState, ()>,
    ) -> GraphResult<NodeResult<WorkflowState, (), String>> {
        let response = format!("Based on: {}", ctx.state.research.as_deref().unwrap_or(""));
        Ok(NodeResult::end(response))
    }
}

let graph = Graph::new()
    .node("research", ResearchNode)
    .node("write", WriteNode)
    .entry("research")
    .build()?;

let result = graph.run(WorkflowState::default(), ()).await?;
```

## ğŸ“¦ Crates

SerdesAI is organized as a workspace of focused crates:

| Crate | Description |
|-------|-------------|
| `serdes-ai` | Main facade with re-exports |
| `serdes-ai-core` | Core types, messages, errors |
| `serdes-ai-agent` | Agent implementation |
| `serdes-ai-models` | Model trait and providers |
| `serdes-ai-providers` | Provider abstractions |
| `serdes-ai-tools` | Tool definitions and execution |
| `serdes-ai-toolsets` | Tool collections and composition |
| `serdes-ai-output` | Output schemas and validation |
| `serdes-ai-streaming` | Streaming support |
| `serdes-ai-mcp` | MCP protocol support |
| `serdes-ai-embeddings` | Embedding models |
| `serdes-ai-retries` | Retry strategies |
| `serdes-ai-graph` | Graph-based workflows |
| `serdes-ai-evals` | Evaluation framework |
| `serdes-ai-macros` | Procedural macros |

## ğŸ”Œ Supported Providers

| Provider | Feature Flag | Models | Status |
|----------|--------------|--------|--------|
| OpenAI | `openai` (default) | GPT-4, GPT-4o, o1, o3 | âœ… Full |
| Anthropic | `anthropic` (default) | Claude 3.5, Claude 4 | âœ… Full |
| Google | `google` (default) | Gemini 1.5, Gemini 2.0 | âœ… Full |
| Groq | `groq` | Llama 3, Mixtral, Gemma | âœ… Full |
| Mistral | `mistral` | Mistral Large, Codestral | âœ… Full |
| Ollama | `ollama` | Any local model | âœ… Full |
| Azure OpenAI | `azure` | Azure-hosted OpenAI | âœ… Full |
| AWS Bedrock | `bedrock` | Claude, Llama, Titan | âœ… Full |

### Provider Examples

```rust
// OpenAI
let model = OpenAIChatModel::from_env("gpt-4o")?;

// Anthropic
let model = AnthropicModel::from_env("claude-3-5-sonnet-20241022")?;

// Google Gemini
let model = GoogleModel::from_env("gemini-1.5-pro")?;

// Groq (ultra-fast inference)
let model = GroqModel::from_env("llama-3.1-70b-versatile")?;

// Mistral
let model = MistralModel::from_env("mistral-large-latest")?;

// Ollama (local)
let model = OllamaModel::new("llama3.1");

// Azure OpenAI
let model = AzureOpenAIModel::from_env("my-deployment")?;

// AWS Bedrock
let model = BedrockModel::new("anthropic.claude-3-sonnet-20240229-v1:0")?;
```

## ğŸ¯ Feature Flags

```toml
[dependencies]
serdes-ai = { version = "0.1", features = ["full"] }
```

| Feature | Description | Default |
|---------|-------------|--------|
| `openai` | OpenAI GPT models | âœ… |
| `anthropic` | Anthropic Claude models | âœ… |
| `google` | Google Gemini models | âœ… |
| `groq` | Groq fast inference | |
| `mistral` | Mistral AI models | |
| `ollama` | Ollama local models | |
| `azure` | Azure OpenAI | |
| `bedrock` | AWS Bedrock | |
| `mcp` | Model Context Protocol | |
| `graph` | Graph workflows | âœ… |
| `evals` | Evaluation framework | âœ… |
| `macros` | Procedural macros | âœ… |
| `full` | All features | |

## ğŸ“– Documentation

- [API Documentation](https://docs.rs/serdes-ai)
- [Examples](./examples/)
- [Migration from pydantic-ai](./docs/migration.md)

## âš–ï¸ Comparison with pydantic-ai

| Feature | pydantic-ai | serdes-ai |
|---------|-------------|----------|
| Language | Python | Rust |
| Type Safety | Runtime (Pydantic) | Compile-time |
| Async Runtime | asyncio | tokio |
| Validation | Pydantic v2 | serde + custom |
| Performance | Good | Excellent |
| Memory Safety | Garbage Collected | Ownership system |
| Binary Size | Large (Python runtime) | Minimal |
| Startup Time | Slow | Instant |
| Thread Safety | GIL limitations | Fully concurrent |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         serdes-ai                           â”‚
â”‚                    (Main Facade Crate)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ serdes-ai-    â”‚    â”‚ serdes-ai-    â”‚    â”‚ serdes-ai-    â”‚
â”‚    agent      â”‚    â”‚    models     â”‚    â”‚    graph      â”‚
â”‚               â”‚    â”‚               â”‚    â”‚               â”‚
â”‚  Agent logic  â”‚    â”‚ Model trait   â”‚    â”‚ Multi-agent   â”‚
â”‚  Run context  â”‚    â”‚ Providers     â”‚    â”‚ workflows     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                     â”‚
                   â–¼                     â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ serdes-ai-    â”‚    â”‚ serdes-ai-    â”‚
          â”‚    tools      â”‚    â”‚    core       â”‚
          â”‚               â”‚    â”‚               â”‚
          â”‚ Tool traits   â”‚    â”‚ Messages      â”‚
          â”‚ Schema gen    â”‚    â”‚ Errors        â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§ª Testing

```bash
# Run all tests
cargo test --workspace --all-features

# Run with specific provider
cargo test --features openai

# Run benchmarks
cargo bench --workspace
```

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines.

### Development Setup

```bash
git clone https://github.com/janfeddersen-wq/serdesAI
cd serdesAI
cargo build --workspace --all-features
cargo test --workspace --all-features
```

## ğŸ“„ License

MIT License - see [LICENSE](./LICENSE) for details.

## ğŸ™ Acknowledgments

- [pydantic-ai](https://github.com/pydantic/pydantic-ai) - The original Python implementation that inspired this project
- [Anthropic](https://anthropic.com) - For Claude and the Model Context Protocol
- [OpenAI](https://openai.com) - For the OpenAI API and tool calling standards
- The Rust community for excellent crates like `tokio`, `serde`, and `async-trait`

---

<p align="center">
  Made with ğŸ¦€ and â¤ï¸
</p>
